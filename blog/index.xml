<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>blog on ᕕ( ᐛ )ᕗ xiaoli's blog</title><link>https://zerlina-ysl.github.io/blog/</link><description>Recent content in blog on ᕕ( ᐛ )ᕗ xiaoli's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 29 Jan 2025 23:01:38 +0800</lastBuildDate><atom:link href="https://zerlina-ysl.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>【go】pdqsort</title><link>https://zerlina-ysl.github.io/blog/pdqsort/</link><pubDate>Wed, 29 Jan 2025 23:01:38 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/pdqsort/</guid><description>1.排序算法 语言 排序算法 python timsort &amp;mdash; &amp;mdash; go1.18 混合排序，主要是快速排序 &amp;mdash; &amp;mdash; rust pdqsort &amp;mdash; &amp;mdash; c++ intro sort 1. 插入排序 将元素不断插入已排序的序列中。即不断交换，直到找到第一个比其小的元素。 2. 堆排序 利用堆的性质,构造一个大顶堆,将最大元素交换到最后一个位置，调整整个堆，如此反复 3. 快速排序 递归，不断分割序列直到序列整体有序。选定一个pivot,使用pivot分割序列，分成元素比pivot大和元素比pivot小的两个序列。
当pivot为中点，快排表现最好；当pivot选择为最大/最小元素时，快排表现最差。 实际场景下，根据序列长度划分后进行benchmark(16/128/1024)
完全随机情况下，短序列时插入排序最快，中、长序列时快排最快，但堆排序和快排的区别不大 完全有序情况下，插入排序最快 任何情况下，堆排序的表现比较稳定 2. pattern-defeating-quicksort 该算法结合三种排序算法的优点：
短序列，使用插入排序 其他情况用快排保证整体性能 当快排表现不佳时，使用堆排序保证最坏情况下的时间复杂度 短序列的长度为多少？12-32，在go语言的实现版本中，将短序列的长度选定为12。 何时从快排切换到堆排序？最终pivot距离序列两端距离小于length/8判定为快排表现不佳，这种情况次数达到bits.Len(length)时，切换为堆排序。bits.Len(length)表达的是一种阈值。
1. 关键点 pivot的选择：需要平衡寻找pivot的成本和pivot带来的性能 首个元素 实现简单，但完全有序情况效果不好 中位数 需遍历数组，性能不好,当array 的长度太长的话，寻找真正的 median 是一个非常昂贵的操作，需要存储所有的 items 近似中位数，根据序列长度不同决定选择策略 短序列(&amp;lt;=8),选择固定元素 中序列(&amp;lt;=50),采样三个元素 长序列(&amp;gt;50),采样九个元素 pivot的采样方式可以让我们探知当前的序列状态 采样元素逆序&amp;ndash;&amp;gt;序列可能逆序&amp;ndash;&amp;gt;翻转序列 采样元素顺序&amp;ndash;&amp;gt;序列可能顺序&amp;ndash;&amp;gt;使用插入排序，有限制次数的插入排序 优化重复度较高的情况 如果两次partition生成的pivot相同，即partition进行了无效分割，此时任务pivot的值为重复元素，limit-1，随机交换元素 2. 函数解析 1. choosePivot：选择pivot 支持三种选择方法：静态选择、三数取中、Tukey&amp;rsquo;s ninther
当数组长度大于等于 8 时，选择1/4、2/4、3/4处，在这三个位置中再找一个中位数作为最终的枢轴 如果长度大于等于 50 时，选择1/4、2/4、3/4处及其每处的相邻元素(共9个元素)的中位数作为最终的枢轴</description></item><item><title>go语言基础</title><link>https://zerlina-ysl.github.io/blog/go_base/</link><pubDate>Sat, 10 Aug 2024 22:03:56 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/go_base/</guid><description>1、gopath go1.15之前，使用绝对路径提供项目的工作目录
bin
存放编译生成的二进制文件。比如 执行命令 go install github.com/google/gops，bin目录会生成 gops 的二进制文件。 pkg
XX_amd64: 其中 XX 是目标操作系统，比如 mac 系统对应的是darwin_amd64, linux 系统对应的是 linux_amd64，存放的是.a结尾的文件。 $GOPATH/pkg/mod: 当开启go Modules 模式下，go get命令缓存下依赖包存放的位置 $GOPATH/pkg/sumdb: go get命令缓存下载的checksum数据存放的位置 GOPATH 模式下，我们需要将应用代码存放在固定的$GOPATH/src目录下，且如果执行go get使用第三方类库的时候会自动下载并安装到$GOPATH目录下。
使用问题 项目地址问题 需要将应用代码存放在固定的$GOPATH/src目录下;第三方套件go get后也需要放置在GOPATH/src的路径下才可以使用。
项目的Golang代码，和第三方的Golang文件混在一起, 项目文件管理麻烦
多个项目时设置多个gopath，不同的GoPath都需要下载依赖，那么磁盘中重复的依赖就会非常多，会占用大量的磁盘空间
2、GOMODULE 使用gomodule管理第三方依赖,将第三方库储存在本地的空间，给项目代码引用
通过GO111MODULE控制：
on/不会去GOPATH目录下查找依赖 auto/根据当前目录来决定是否启用modules功能 off go.mod记录依赖包，go.sum保证依赖版本不被篡改
3、基本命令 go.mod Go 命令行工具会自动处理 go.mod 中指定的模块版本。当源代码中 import 指向的模块不存在于 go.mod 文件中时，Go 命令行工具会自动搜索这个模块，并将最新版本（最后一个 tag 且非预发布的稳定版本）添加到 go.mod 文件中。
go.sum https://eddycjy.com/posts/go/go-moduels/2019-09-29-goproxy-cn/
详细罗列了当前项目直接或间接依赖的所有模块版本，并写明了模块版本的 SHA-256 哈希值以备 Go 在今后的操作中保证项目所依赖的那些模块版本不会被篡改。
# Go modules 打包整个模块包文件 zip 后再进行 hash 值 icode.</description></item><item><title>记录mit6.824-lab1</title><link>https://zerlina-ysl.github.io/blog/6.824_lab1/</link><pubDate>Mon, 27 Nov 2023 23:54:28 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/6.824_lab1/</guid><description>环境准备 https://pdos.csail.mit.edu/6.824/notes/l01.txt
http://nil.csail.mit.edu/6.824/2022/labs/lab-mr.html
Go version:http://nil.csail.mit.edu/6.824/2022/labs/go.html
本质 建议把示例中的调用逻辑整理清晰，并把相关的描述文档多看几遍。
lab1本质上由单worker单coordinator变成三个worker单coodinator，处理好并发安全就可以。
go语言处理并发安全无非那老一套：使用channel通信；加锁。
流程图 使用飞书画的，好可爱!!!
脚本测试过程 // 构建插件 go build -race -buildmode=plugin ../mrapps/wc.go rm mr-out* // 每个文件就是map的输入 go run -race mrcoordinator.go pg-*.txt go build -race -buildmode=plugin ../mrapps/wc.go go run -race mrworker.go wc.so cat mr-out-* | sort | more -------- // 也可以直接运行： bash test-mr.sh 测试共包含8个，全部PASS算通过；建议将不通过的脚本单独复制出来测试。
代码 https://github.com/Zerlina-ysl/6.824
参考 https://blog.csdn.net/weixin_45938441/article/details/124018485</description></item><item><title>浅浅学习分布式算法</title><link>https://zerlina-ysl.github.io/blog/distrubute_algorithms/</link><pubDate>Mon, 06 Mar 2023 16:18:03 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/distrubute_algorithms/</guid><description>Paxos Paxos是一个共识(consensus)算法，共识算法解决的是分布式系统对某个提案（Proposal），大部分节点达成一致意见的过程。 保证最终只有一个提案会被选定，提案被选定后进程也可以获取到被选定的提案。
前提 假设不同参与者可以通过发送消息来通信，并使用普通的非拜占庭模式的异步模型，即每个参与者以任意速度执行，可能会出错而停止，也可能会重启,消息在传输中可能会花费任意的时间，可能会重复、丢失，但不会被损坏，即其内容不会被篡改，不会发生拜占庭式问题。
拜占庭将军问题：某国有许多军队，军队的将军需要制定一个统一的行动计划&amp;mdash;进攻或撤退。将军们地理位置是分开的，只能靠通讯员通信，将军中存在叛徒，叛徒可以篡改消息，欺骗将军。~~理论研究显示，在3n+1的系统中，只有叛徒数目小于等于n，才有可能设计一个协议使得叛徒无论怎样作梗也能达成一致。~~即分布式计算中系统中的成员计算机可能出错而发送错误的信息，用于传递信息的通讯网络也可能导致信息损坏，使得网络中不同的成员关于全体协作的策略得出不同结论，从而破坏系统一致性
Paxos算法假设所有信息都是完整的，没有被篡改和伪造。
主要术语 Proposer：提案者
Proposal：提案，由Proposer提出，一个提案由一个编号及value形成的对组成，编号是为了防止混淆，保证提案的可区分性，value代表提案本身的内容。提案的含义在分布式系统中十分宽泛，如多个事件发生的顺序、某个键对应的值、谁是主节点……等等。可以认为任何可以达成一致的信息都是一个提案。
Acceptor：提案的受理者，被动接受来自Proposer的提议，有权决定是否接受该提案。
Choose：被选定的提案，当有半数以上Acceptor接受该提案，就认为提案被选定。
Learner：被动接受来自Acceptor的消息，需要知道被选定的提案信息。
假设有一组可以提出提案的进程集合，一个一致性算法需要保证以下三点：
被提出的提案只有一个会被选定
如果没有提案被提出，就不会有被选定的提案。
当一个提案被选定后，进程可以获取被选定的提案信息。
安全性原则 安全性是指那些需要保证永远都不会发生的事情，
只有被提出的提案可以被选定
只能有一个value被选定
如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定那个。
存活原则 存活是指那些最终一定会发生的事情
最终会批准某个提案的value
一个value被批准了，其他服务器最终会学习到这个value
Basic Paxos Paxos一致性算法分为两阶段提交：prepare阶段和Acceptor阶段
Prepare阶段 阶段 a：proposer选择一个提案编号n，并将携带编号n的prepare请求发送给大多数的Acceptor。
阶段b：Acceptor收到Prepare消息后，如果提案编号大于它已经回复的所有prepare消息，Acceptor回复给Proposer一个promise消息，承诺不再接受小于n的提案。promise携带acceptor接受过的最大的编号。
Acceptor阶段 阶段a: 如果proposer从大多数acceptor中接收到对prepare请求的回复，它将向这些acceptors发送一个编号为n，value为v的accept请求，v是这些回复中最高编号的提案，或者响应报告中没有提案，v是任意值。
阶段b: 如果一个acceptor收到了编号为n的acceptor请求，它会接受该提案。除非已经回复过编号大于n的prepare请求。
示例 P2分别向A2 A3发送序号为2的prepare请求，A2 A3之前没有接受过序号更大的prepare消息，因此会向P2返回一个序号为2的promise请求。
P1分别向A1 A2发送序号为1的prepare请求,A1之前没有接受过序号更大的prepare消息，因此会向P1返回一个序号为1的promise请求。而A2已经接受过序号为2的，因此会忽略P1的prepare请求。
P1为了将提案达成共识，会发送一个Acceptor请求，携带编号和value。
P2为了将提案达成共识，会发生Acceptor请求，携带编号和value。
A1会将共识发送给L1，并回复P1。
A2 A3 会将共识发送给L1，并回复P2。
活锁问题 P1提交的Proposal被拒绝时，可能存在因为Acceptor承诺返回了更大编号的Proposal，P1提高Proposal编号继续提交。一旦出现这种情况，两个Proposer都发现自己的编号过低转而提出更高编号的Proposal，这会导致死循环，该现象被称为活锁。你编号高，我再比你更高，反复如此，算法永远无法结束。
对于Basic Paxos来说，具有活锁问题，每次只能对单个提案形成决议，而决议的形成至少需要两次网络请求和应答，如果每个命令都需要通过Basic Paxos算法达到一致，会产生大量开销。 Basic Paxos的价值在于开拓了分布式共识算法的发展思路，但无法直接用于实践，只能作为学术研究。但是正如Mike Burrows所说，”There is only one consensus protocol, and that&amp;rsquo;s “Paxos” — all other approaches a re just broken versions of Paxos“，许多实际应用的分布式算法如Raft、ZAB等都是基于Basic Paxos的改进。</description></item><item><title>云原生之k8s入门</title><link>https://zerlina-ysl.github.io/blog/devops_k8s/</link><pubDate>Wed, 22 Feb 2023 20:16:40 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/devops_k8s/</guid><description>在学习docker的时候我们说到，docker的使用标志着进入容器化时代，但随之也产生了许多问题：
如何协调和调度这些容器？ 如何保证升级应用时不会中断服务的提供？ 如何监控容器的运行状况？ 如何批量操作容器中的应用？ &amp;hellip;&amp;hellip; 解决这些问题所需要的就是容器编排技术，k8s是当前流行的大规模容器编排系统。
服务发现和负载均衡。使用DNS名称或ip地址公开容器，k8s可以负载均衡并分配网络流量。 存储编排。允许自动挂载所选择的存储系统，如本地存储、公共云提供商等。 自动部署和回滚。可以自动化k8s创建新容、删除现有容器并将资源用于新容器 自动装箱计算。k8s可以管理容器所需的cpu和内存(RAM) 自我修复。 密钥和配置管理。k8s允许存储和管理敏感信息，可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，无需在堆栈配置中暴露密钥。 k8s提供了弹性运行分布式系统框架，满足扩展要求、故障转移、部署模式等。 架构 https://kubernetes.io/docs/concepts/overview/components/ 集群部署 k8s的集群创建需要最少三台机器，我在阿里云购买了三台centos服务器，对于服务器有以下要求。
2cpu或更多 2GB或更多RAM 机器间可以互相通信、不能有重复的主机名、mac地址 禁用交换分区 对于三台服务器，创建了网段为171.31.0.0/16的专用网络，和网段为171.31.0.0/24的交换机，并让三台服务器位于该专用网络和交换机下。
以下命令默认在所有机器中都要执行
1. 安装kubeadm 配置基础环境 #各个机器设置自己的域名 hostnamectl set-hostname xxxx # 刷新，显示主机名 bash # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i &amp;#39;s/^SELINUX=enforcing$/SELINUX=permissive/&amp;#39; /etc/selinux/config #关闭swap swapoff -a sed -ri &amp;#39;s/.*swap.*/#&amp;amp;/&amp;#39; /etc/fstab #允许 iptables 检查桥接流量 cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.</description></item><item><title>云原生之docker入门</title><link>https://zerlina-ysl.github.io/blog/devops_docker/</link><pubDate>Tue, 21 Feb 2023 21:03:09 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/devops_docker/</guid><description>是什么 通过对应用组件的封装、分发、部署、运行等生命周期的管理，使用户的应用程序及其运行环境能做到一次镜像,处处运行。
为什么 传统虚拟机技术基于安装在主操作系统上的虚拟机管理系统（如VirtualBox、VMware等），创建虚拟机（虚拟出各种硬件），在虚拟机上安装从操作系统，在从操作系统中安装部署各种应用。
缺点：资源占用多、冗余步骤多、启动慢
Linux容器不是模拟一个完整的操作系统，而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。容器与虚拟机不同，不需要捆绑一整套操作系统，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一的运行。
主要概念 容器：容器可以独立运行应用，应用程序或服务运行在容器里面，容器就类似于一个虚拟化的运行环境，容器是用镜像创建的运行实例。每个容器相互隔离。
镜像：一个只读的模板。镜像可以用来创建Docker容器，一个镜像可以创建多个容器。一般会把可运行系统打包成一个镜像。
仓库：集中存放镜像文件的场所。https://hub.docker.com/
docker_host：安装docker的主机
docker deamon： 守护进程，运行在docker主机上的docker后台进程。
​ 守护进程：在后台运行并提供某种服务的进程，它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。
docker本质上是一个容器运行载体，我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个运行环境就是image镜像文件，通过该镜像文件生成docker容器实例。
从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graph Driver 将下载镜像以 Graph 的形式存储 通过网络管理驱动 Network driver 创建并配置 Docker 容器网络环境 通过 Exec driver 来完成限制 Docker 容器运行资源或执行用户指令等 Libcontainer 是一项独立的容器管理包，Network driver 以及 Exec driver 都是通过Libcontainer 来实现具体对容器进行的操作 docker命令实战 安装docker 其他系统参照如下文档 https://docs.docker.com/engine/install/centos/
移除其他docker安装包 sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine 配置yum源 sudo yum install -y yum-utils sudo yum-config-manager \ --add-repo \ http://mirrors.</description></item><item><title>自动化流水线如何构建项目</title><link>https://zerlina-ysl.github.io/blog/how_bytecycle_run/</link><pubDate>Sat, 19 Nov 2022 23:15:09 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/how_bytecycle_run/</guid><description>自动化流水线如何构建项目并运行？
DevOps（Development和Operations的组合词）是一种重视“软件开发人员（Dev）”和“IT运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。透过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。
SourceControlManagement 代码发布管理平台 拉取源代码，运行指定的build脚本(如 sh build.sh），将打包后的output目录下的内容打包成tar包并上传，生成可以在线上运行的代码或资源。
CloudEngine云引擎 容器发布和管理平台，基于k8s。 到scm仓库拉取tar的压缩包，根据配置生成用于构建的dockerFile，构建docker镜像,挂在宿主机上，启动服务，为服务创建唯一标识（psm)进行后续的管理。部署时通过k8s调度到某个node的pod，下载服务镜像。
TCE将一些物理服务器资源合并成一个集群，再将集群拆分为更细粒度的计算资源，从而实现对计算资源更灵活的调度和分配。
集群中多实例部署，每个实例只需要负责一部分的请求流量。多实例也可以实现容灾和滚动升级。
可以通过实例 ip+port的方法访问所部署的服务，也可以使用psm，即每个实例启动，会向注册中心报告自己的psm和ip，这样下游服务就可以访问注册中心来获得对应psm的实例列表。
ApiManament 管理服务接口,集成了测试、抓包、mock测试等功能。
ApiGeteWay 以API为核心，提供流量调度、限流、降级、服务治理等全套解决方案能力的网关平台。 AGW将http协议转换为thrift协议，把http请求转换为kitex请求。所以也需要在idl文件中根据idl语法进行标注。
分布式AGW 网关以独立的进程部署在业务实例中，流量-&amp;gt;TLB-&amp;gt;业务集群，通过网关转发到业务实例。流量路径中少一跳，整体时延收益明显。同时，分布式网关以sidecar模式独立进程部署在业务实例中，业务PSM天然隔离，隔离性和稳定性更好。 分布式AGW不支持跨机房流量调度、分流能力，比较依赖tlb的分流能力；不支持请求合并。
中心化AGW 流量路径从TLB–&amp;gt;网关集群–&amp;gt;业务集群。优势在于AGW集群作为集中到网关节点，拥有灵活的流量调度能力，如集群分流、机房调度等。
网关逻辑的更改发布一旦有问题，会影响所有业务。
从网关能力演进的角度来看，分布式网关是未来的趋势。API Gateway Mesh逐渐成为主流。
LoadBalance 管理服务域名流量入口的七层负载均衡服务。
用户可以在TLB平台为自己的服务配置对外的域名从而完成流量接入。支持对域名进行匹配，如正则匹配、自定义匹配等，将访问域名的流量通过不同路由分发到对应的后端psm服务。
参考 相爱相杀：Servicemesh和API Gateway关系深度探讨：https://skyao.io/post/202004-servicemesh-and-api-gateway/
蚂蚁金服 API Gateway Mesh 思考与实践：https://www.infoq.cn/article/azCFGyTDGakZqaLEEDMN</description></item><item><title>浅谈微服务架构</title><link>https://zerlina-ysl.github.io/blog/microservice/</link><pubDate>Fri, 18 Nov 2022 23:06:45 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/microservice/</guid><description>是什么？ 微服务以专注于单一责任与功能的小型功能区块为基础，利用模块化组合出复杂的大型应用程序，各区块可以使用与语言无关的api相互通讯。 服务框架的构建是一个持续演进的过程。
单体架构 最初的服务，如实现一个小说app，我们会把所有的功能都实现在一个程序，将各种功能按照高内聚低耦合的理念划分成多个模块(书成模块、登陆模块、书架模块等等…)。一个服务就是一个应用。
随着访问量增多，服务器压力变大，为了支持业务发展，首先选择了最直接的方法—加机器。
因此需要负载均衡来支持流量增长。所有的请求需要通过一层负载均衡，再打到最终的服务上。
但随着业务的发展、产品的迭代，单体架构的缺陷愈发明显：
所有功能高度耦合，互相影响，难以管理，团队合作受限 部分的逻辑修改也需要对整体服务进行开发和测试，采用瀑布式开发模型，开发周期长迭代慢。且需要影响全部服务的重建和部署，成本高 一个小bug可能会导致整个系统的崩溃
因此，需要对单体架构进行解耦！ 面向服务 借助单体架构的功能划分，将单体架构划分为几个大的服务应用。 每个服务应用内部通过负载均衡横向扩展。 拆分后的每个服务应用创建自己的接口定义，以便被其他服务发现 服务之间的通讯类似于计租中的总线原理：通过一个独立的中间件提供消息通讯
以上是简易版企业服务总线的初步设计，系统的所有流量通过通信总线进行路由分发。但随着流量的激增，通信总线会出现问题，通信总线成为了整个系统的中心化节点和瓶颈，影响了系统的扩展性和稳定性。 SOA 去中心化。
对于通信总线分发流量而导致的问题，使用DNS+IP网络架构解决。
域名：为每个服务定义一个psm(product.subsystem.module)作为服务的域名 IP：服务实例其实就是一个机器终端，拥有自己的ip地址 根服务器： 维护psm-&amp;gt;ip的映射，实现服务发现 服务注册：每个服务实例在创建后不断的向注册中心上报自己的地址(psm-ip)和实例状态 服务治理：注册中心维护着每个服务对应的实例地址列表及其健康状态 服务发现：实例定时向注册中心获取要访问的服务的实例地址，访问时通过负载均衡选择其中一个p2p来访问，流量不经由注册中心转发。负载均衡通过服务本身来实现。 通信协议：服务提供IDL文件，通过统一的rpc进行通信。 引入gateway将外部流量转换为内部请求。注册中心可以多实例部署，集群内部使用分布式算法保证其最终一致性。可以对服务进行更细粒度的拆分。
一个注册中心所具有的基本功能： 注册服务实例信息；心跳机制；剔除失败的服务实例；查询服务实例信息。
如何注册和发现服务？ 在微服务架构下，主要有三种角色：服务提供者（RPC Server）、服务消费者（RPC Client）和服务注册中心（Registry）。
RPC Server 提供服务，在启动时，根据服务发布文件 server.xml 中的配置的信息，向 Registry 注册自身服务，并向 Registry 定期发送心跳汇报存活状态。
RPC Client 调用服务，在启动时，根据服务引用文件 client.xml 中配置的信息，向 Registry 订阅服务，把 Registry 返回的服务节点列表缓存在本地内存中，并与 RPC Sever 建立连接。
当 RPC Server 节点发生变更时，Registry 会同步变更，RPC Client 感知后会刷新本地内存中缓存的服务节点列表。
RPC Client 从本地缓存的服务节点列表中，基于负载均衡算法选择一台 RPC Sever 发起调用。</description></item><item><title>使用git rebase合并冲突</title><link>https://zerlina-ysl.github.io/blog/git_rebase/</link><pubDate>Tue, 15 Nov 2022 00:18:23 +0800</pubDate><guid>https://zerlina-ysl.github.io/blog/git_rebase/</guid><description>今天在公司使用git rebase master解冲突失败，同事悉心教我，下来再研究一下。
git rebase master 回退分支 这一步可以跳过，只是我错误rebase操作导致目前分支代码不正常，需要先回滚代码。
git log commit b77bd104afddea0cb0c861dd03ecd3350848xxxx (HEAD -&amp;gt; i18n_author_guide, origin/i18n_author_guide) Author: lishenyu &amp;lt;lishenyu@xx.com&amp;gt; Date: Mon Nov 14 20:16:42 2022 +0800 fix: rebase master commit 1afc280edcc522f1749112476a0406ba0683xxxx Author: zheng &amp;lt;zheng@xx.com&amp;gt; Date: Wed Nov 9 14:14:16 2022 +0800 fix: idl更新 可以看到，我需要通过logID回退到历史版本。
git reset --hard 1afc280edcc522f1749112476a0406ba0683xxxx HEAD is now at 1afc280 fix: idl更新 git reset实现回退操作。首先需要明确三个区域：工作区/ 暂存区/ Repository。
我们对工作区中的代码进行修改，使用git add .将修改的所有代码添加到暂存区，使用git push将暂存区中的代码提交到远程的Repository。
–hard 撤销工作区所有未提交修改，将暂存区和工作区都回到上一个版本，删除之前的提交信息 –soft 保留工作目录中的内容，把新内容放置在暂存区。 –mixed 所有修改放在工作目录 ⚠️：git reset后再次push会引发冲突，需要push -f 1.</description></item></channel></rss>