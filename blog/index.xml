<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>blog on ᕕ( ᐛ )ᕗ xiaoli&#39;s blog</title>
    <link>https://zerlina-ysl.github.io/blog/</link>
    <description>Recent content in blog on ᕕ( ᐛ )ᕗ xiaoli&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Nov 2023 23:54:28 +0800</lastBuildDate><atom:link href="https://zerlina-ysl.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>记录mit6.824-lab1</title>
      <link>https://zerlina-ysl.github.io/blog/6.824_lab1/</link>
      <pubDate>Mon, 27 Nov 2023 23:54:28 +0800</pubDate>
      
      <guid>https://zerlina-ysl.github.io/blog/6.824_lab1/</guid>
      <description>环境准备 https://pdos.csail.mit.edu/6.824/notes/l01.txt
http://nil.csail.mit.edu/6.824/2022/labs/lab-mr.html
Go version:http://nil.csail.mit.edu/6.824/2022/labs/go.html
本质 建议把示例中的调用逻辑整理清晰，并把相关的描述文档多看几遍。
lab1本质上由单worker单coordinator变成三个worker单coodinator，处理好并发安全就可以。
go语言处理并发安全无非那老一套：使用channel通信；加锁。
流程图 使用飞书画的，好可爱!!!
脚本测试过程 // 构建插件 go build -race -buildmode=plugin ../mrapps/wc.go rm mr-out* // 每个文件就是map的输入 go run -race mrcoordinator.go pg-*.txt go build -race -buildmode=plugin ../mrapps/wc.go go run -race mrworker.go wc.so cat mr-out-* | sort | more -------- // 也可以直接运行： bash test-mr.sh 测试共包含8个，全部PASS算通过；建议将不通过的脚本单独复制出来测试。
代码 https://github.com/Zerlina-ysl/6.824
参考 https://blog.csdn.net/weixin_45938441/article/details/124018485</description>
    </item>
    
    <item>
      <title>浅浅学习分布式算法</title>
      <link>https://zerlina-ysl.github.io/blog/distrubute_algorithms/</link>
      <pubDate>Mon, 06 Mar 2023 16:18:03 +0800</pubDate>
      
      <guid>https://zerlina-ysl.github.io/blog/distrubute_algorithms/</guid>
      <description>Paxos Paxos是一个共识(consensus)算法，共识算法解决的是分布式系统对某个提案（Proposal），大部分节点达成一致意见的过程。 保证最终只有一个提案会被选定，提案被选定后进程也可以获取到被选定的提案。
前提 假设不同参与者可以通过发送消息来通信，并使用普通的非拜占庭模式的异步模型，即每个参与者以任意速度执行，可能会出错而停止，也可能会重启,消息在传输中可能会花费任意的时间，可能会重复、丢失，但不会被损坏，即其内容不会被篡改，不会发生拜占庭式问题。
拜占庭将军问题：某国有许多军队，军队的将军需要制定一个统一的行动计划&amp;mdash;进攻或撤退。将军们地理位置是分开的，只能靠通讯员通信，将军中存在叛徒，叛徒可以篡改消息，欺骗将军。~~理论研究显示，在3n+1的系统中，只有叛徒数目小于等于n，才有可能设计一个协议使得叛徒无论怎样作梗也能达成一致。~~即分布式计算中系统中的成员计算机可能出错而发送错误的信息，用于传递信息的通讯网络也可能导致信息损坏，使得网络中不同的成员关于全体协作的策略得出不同结论，从而破坏系统一致性
Paxos算法假设所有信息都是完整的，没有被篡改和伪造。
主要术语 Proposer：提案者
Proposal：提案，由Proposer提出，一个提案由一个编号及value形成的对组成，编号是为了防止混淆，保证提案的可区分性，value代表提案本身的内容。提案的含义在分布式系统中十分宽泛，如多个事件发生的顺序、某个键对应的值、谁是主节点……等等。可以认为任何可以达成一致的信息都是一个提案。
Acceptor：提案的受理者，被动接受来自Proposer的提议，有权决定是否接受该提案。
Choose：被选定的提案，当有半数以上Acceptor接受该提案，就认为提案被选定。
Learner：被动接受来自Acceptor的消息，需要知道被选定的提案信息。
假设有一组可以提出提案的进程集合，一个一致性算法需要保证以下三点：
被提出的提案只有一个会被选定
如果没有提案被提出，就不会有被选定的提案。
当一个提案被选定后，进程可以获取被选定的提案信息。
安全性原则 安全性是指那些需要保证永远都不会发生的事情，
只有被提出的提案可以被选定
只能有一个value被选定
如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定那个。
存活原则 存活是指那些最终一定会发生的事情
最终会批准某个提案的value
一个value被批准了，其他服务器最终会学习到这个value
Basic Paxos Paxos一致性算法分为两阶段提交：prepare阶段和Acceptor阶段
Prepare阶段 阶段 a：proposer选择一个提案编号n，并将携带编号n的prepare请求发送给大多数的Acceptor。
阶段b：Acceptor收到Prepare消息后，如果提案编号大于它已经回复的所有prepare消息，Acceptor回复给Proposer一个promise消息，承诺不再接受小于n的提案。promise携带acceptor接受过的最大的编号。
Acceptor阶段 阶段a: 如果proposer从大多数acceptor中接收到对prepare请求的回复，它将向这些acceptors发送一个编号为n，value为v的accept请求，v是这些回复中最高编号的提案，或者响应报告中没有提案，v是任意值。
阶段b: 如果一个acceptor收到了编号为n的acceptor请求，它会接受该提案。除非已经回复过编号大于n的prepare请求。
示例 P2分别向A2 A3发送序号为2的prepare请求，A2 A3之前没有接受过序号更大的prepare消息，因此会向P2返回一个序号为2的promise请求。
P1分别向A1 A2发送序号为1的prepare请求,A1之前没有接受过序号更大的prepare消息，因此会向P1返回一个序号为1的promise请求。而A2已经接受过序号为2的，因此会忽略P1的prepare请求。
P1为了将提案达成共识，会发送一个Acceptor请求，携带编号和value。
P2为了将提案达成共识，会发生Acceptor请求，携带编号和value。
A1会将共识发送给L1，并回复P1。
A2 A3 会将共识发送给L1，并回复P2。
活锁问题 P1提交的Proposal被拒绝时，可能存在因为Acceptor承诺返回了更大编号的Proposal，P1提高Proposal编号继续提交。一旦出现这种情况，两个Proposer都发现自己的编号过低转而提出更高编号的Proposal，这会导致死循环，该现象被称为活锁。你编号高，我再比你更高，反复如此，算法永远无法结束。
对于Basic Paxos来说，具有活锁问题，每次只能对单个提案形成决议，而决议的形成至少需要两次网络请求和应答，如果每个命令都需要通过Basic Paxos算法达到一致，会产生大量开销。 Basic Paxos的价值在于开拓了分布式共识算法的发展思路，但无法直接用于实践，只能作为学术研究。但是正如Mike Burrows所说，”There is only one consensus protocol, and that&amp;rsquo;s “Paxos” — all other approaches a re just broken versions of Paxos“，许多实际应用的分布式算法如Raft、ZAB等都是基于Basic Paxos的改进。</description>
    </item>
    
    <item>
      <title>云原生之k8s入门</title>
      <link>https://zerlina-ysl.github.io/blog/devops_k8s/</link>
      <pubDate>Wed, 22 Feb 2023 20:16:40 +0800</pubDate>
      
      <guid>https://zerlina-ysl.github.io/blog/devops_k8s/</guid>
      <description>在学习docker的时候我们说到，docker的使用标志着进入容器化时代，但随之也产生了许多问题：
如何协调和调度这些容器？ 如何保证升级应用时不会中断服务的提供？ 如何监控容器的运行状况？ 如何批量操作容器中的应用？ &amp;hellip;&amp;hellip; 解决这些问题所需要的就是容器编排技术，k8s是当前流行的大规模容器编排系统。
服务发现和负载均衡。使用DNS名称或ip地址公开容器，k8s可以负载均衡并分配网络流量。 存储编排。允许自动挂载所选择的存储系统，如本地存储、公共云提供商等。 自动部署和回滚。可以自动化k8s创建新容、删除现有容器并将资源用于新容器 自动装箱计算。k8s可以管理容器所需的cpu和内存(RAM) 自我修复。 密钥和配置管理。k8s允许存储和管理敏感信息，可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，无需在堆栈配置中暴露密钥。 k8s提供了弹性运行分布式系统框架，满足扩展要求、故障转移、部署模式等。 架构 https://kubernetes.io/docs/concepts/overview/components/ 集群部署 k8s的集群创建需要最少三台机器，我在阿里云购买了三台centos服务器，对于服务器有以下要求。
2cpu或更多 2GB或更多RAM 机器间可以互相通信、不能有重复的主机名、mac地址 禁用交换分区 对于三台服务器，创建了网段为171.31.0.0/16的专用网络，和网段为171.31.0.0/24的交换机，并让三台服务器位于该专用网络和交换机下。
以下命令默认在所有机器中都要执行
1. 安装kubeadm 配置基础环境 #各个机器设置自己的域名 hostnamectl set-hostname xxxx # 刷新，显示主机名 bash # 将 SELinux 设置为 permissive 模式（相当于将其禁用） sudo setenforce 0 sudo sed -i &amp;#39;s/^SELINUX=enforcing$/SELINUX=permissive/&amp;#39; /etc/selinux/config #关闭swap swapoff -a sed -ri &amp;#39;s/.*swap.*/#&amp;amp;/&amp;#39; /etc/fstab #允许 iptables 检查桥接流量 cat &amp;lt;&amp;lt;EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat &amp;lt;&amp;lt;EOF | sudo tee /etc/sysctl.</description>
    </item>
    
    <item>
      <title>自动化流水线如何构建项目</title>
      <link>https://zerlina-ysl.github.io/blog/how_bytecycle_run/</link>
      <pubDate>Sat, 19 Nov 2022 23:15:09 +0800</pubDate>
      
      <guid>https://zerlina-ysl.github.io/blog/how_bytecycle_run/</guid>
      <description>自动化流水线如何构建项目并运行？
DevOps（Development和Operations的组合词）是一种重视“软件开发人员（Dev）”和“IT运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。透过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。
SourceControlManagement 代码发布管理平台 拉取源代码，运行指定的build脚本(如 sh build.sh），将打包后的output目录下的内容打包成tar包并上传，生成可以在线上运行的代码或资源。
CloudEngine云引擎 容器发布和管理平台，基于k8s。 到scm仓库拉取tar的压缩包，根据配置生成用于构建的dockerFile，构建docker镜像,挂在宿主机上，启动服务，为服务创建唯一标识（psm)进行后续的管理。部署时通过k8s调度到某个node的pod，下载服务镜像。
TCE将一些物理服务器资源合并成一个集群，再将集群拆分为更细粒度的计算资源，从而实现对计算资源更灵活的调度和分配。
集群中多实例部署，每个实例只需要负责一部分的请求流量。多实例也可以实现容灾和滚动升级。
可以通过实例 ip+port的方法访问所部署的服务，也可以使用psm，即每个实例启动，会向注册中心报告自己的psm和ip，这样下游服务就可以访问注册中心来获得对应psm的实例列表。
ApiManament 管理服务接口,集成了测试、抓包、mock测试等功能。
ApiGeteWay 以API为核心，提供流量调度、限流、降级、服务治理等全套解决方案能力的网关平台。 AGW将http协议转换为thrift协议，把http请求转换为kitex请求。所以也需要在idl文件中根据idl语法进行标注。
分布式AGW 网关以独立的进程部署在业务实例中，流量-&amp;gt;TLB-&amp;gt;业务集群，通过网关转发到业务实例。流量路径中少一跳，整体时延收益明显。同时，分布式网关以sidecar模式独立进程部署在业务实例中，业务PSM天然隔离，隔离性和稳定性更好。 分布式AGW不支持跨机房流量调度、分流能力，比较依赖tlb的分流能力；不支持请求合并。
中心化AGW 流量路径从TLB–&amp;gt;网关集群–&amp;gt;业务集群。优势在于AGW集群作为集中到网关节点，拥有灵活的流量调度能力，如集群分流、机房调度等。
网关逻辑的更改发布一旦有问题，会影响所有业务。
从网关能力演进的角度来看，分布式网关是未来的趋势。API Gateway Mesh逐渐成为主流。
LoadBalance 管理服务域名流量入口的七层负载均衡服务。
用户可以在TLB平台为自己的服务配置对外的域名从而完成流量接入。支持对域名进行匹配，如正则匹配、自定义匹配等，将访问域名的流量通过不同路由分发到对应的后端psm服务。
参考 相爱相杀：Servicemesh和API Gateway关系深度探讨：https://skyao.io/post/202004-servicemesh-and-api-gateway/
蚂蚁金服 API Gateway Mesh 思考与实践：https://www.infoq.cn/article/azCFGyTDGakZqaLEEDMN</description>
    </item>
    
    <item>
      <title>浅谈微服务架构</title>
      <link>https://zerlina-ysl.github.io/blog/microservice/</link>
      <pubDate>Fri, 18 Nov 2022 23:06:45 +0800</pubDate>
      
      <guid>https://zerlina-ysl.github.io/blog/microservice/</guid>
      <description>是什么？ 微服务以专注于单一责任与功能的小型功能区块为基础，利用模块化组合出复杂的大型应用程序，各区块可以使用与语言无关的api相互通讯。 服务框架的构建是一个持续演进的过程。
单体架构 最初的服务，如实现一个小说app，我们会把所有的功能都实现在一个程序，将各种功能按照高内聚低耦合的理念划分成多个模块(书成模块、登陆模块、书架模块等等…)。一个服务就是一个应用。
随着访问量增多，服务器压力变大，为了支持业务发展，首先选择了最直接的方法—加机器。
因此需要负载均衡来支持流量增长。所有的请求需要通过一层负载均衡，再打到最终的服务上。
但随着业务的发展、产品的迭代，单体架构的缺陷愈发明显：
所有功能高度耦合，互相影响，难以管理，团队合作受限 部分的逻辑修改也需要对整体服务进行开发和测试，采用瀑布式开发模型，开发周期长迭代慢。且需要影响全部服务的重建和部署，成本高 一个小bug可能会导致整个系统的崩溃
因此，需要对单体架构进行解耦！ 面向服务 借助单体架构的功能划分，将单体架构划分为几个大的服务应用。 每个服务应用内部通过负载均衡横向扩展。 拆分后的每个服务应用创建自己的接口定义，以便被其他服务发现 服务之间的通讯类似于计租中的总线原理：通过一个独立的中间件提供消息通讯
以上是简易版企业服务总线的初步设计，系统的所有流量通过通信总线进行路由分发。但随着流量的激增，通信总线会出现问题，通信总线成为了整个系统的中心化节点和瓶颈，影响了系统的扩展性和稳定性。 SOA 去中心化。
对于通信总线分发流量而导致的问题，使用DNS+IP网络架构解决。
域名：为每个服务定义一个psm(product.subsystem.module)作为服务的域名 IP：服务实例其实就是一个机器终端，拥有自己的ip地址 根服务器： 维护psm-&amp;gt;ip的映射，实现服务发现 服务注册：每个服务实例在创建后不断的向注册中心上报自己的地址(psm-ip)和实例状态 服务治理：注册中心维护着每个服务对应的实例地址列表及其健康状态 服务发现：实例定时向注册中心获取要访问的服务的实例地址，访问时通过负载均衡选择其中一个p2p来访问，流量不经由注册中心转发。负载均衡通过服务本身来实现。 通信协议：服务提供IDL文件，通过统一的rpc进行通信。 引入gateway将外部流量转换为内部请求。注册中心可以多实例部署，集群内部使用分布式算法保证其最终一致性。可以对服务进行更细粒度的拆分。
一个注册中心所具有的基本功能： 注册服务实例信息；心跳机制；剔除失败的服务实例；查询服务实例信息。
如何注册和发现服务？ 在微服务架构下，主要有三种角色：服务提供者（RPC Server）、服务消费者（RPC Client）和服务注册中心（Registry）。
RPC Server 提供服务，在启动时，根据服务发布文件 server.xml 中的配置的信息，向 Registry 注册自身服务，并向 Registry 定期发送心跳汇报存活状态。
RPC Client 调用服务，在启动时，根据服务引用文件 client.xml 中配置的信息，向 Registry 订阅服务，把 Registry 返回的服务节点列表缓存在本地内存中，并与 RPC Sever 建立连接。
当 RPC Server 节点发生变更时，Registry 会同步变更，RPC Client 感知后会刷新本地内存中缓存的服务节点列表。
RPC Client 从本地缓存的服务节点列表中，基于负载均衡算法选择一台 RPC Sever 发起调用。</description>
    </item>
    
    <item>
      <title>使用git rebase合并冲突</title>
      <link>https://zerlina-ysl.github.io/blog/git_rebase/</link>
      <pubDate>Tue, 15 Nov 2022 00:18:23 +0800</pubDate>
      
      <guid>https://zerlina-ysl.github.io/blog/git_rebase/</guid>
      <description>今天在公司使用git rebase master解冲突失败，同事悉心教我，下来再研究一下。
git rebase master 回退分支 这一步可以跳过，只是我错误rebase操作导致目前分支代码不正常，需要先回滚代码。
git log commit b77bd104afddea0cb0c861dd03ecd3350848xxxx (HEAD -&amp;gt; i18n_author_guide, origin/i18n_author_guide) Author: lishenyu &amp;lt;lishenyu@xx.com&amp;gt; Date: Mon Nov 14 20:16:42 2022 +0800 fix: rebase master commit 1afc280edcc522f1749112476a0406ba0683xxxx Author: zheng &amp;lt;zheng@xx.com&amp;gt; Date: Wed Nov 9 14:14:16 2022 +0800 fix: idl更新 可以看到，我需要通过logID回退到历史版本。
git reset --hard 1afc280edcc522f1749112476a0406ba0683xxxx HEAD is now at 1afc280 fix: idl更新 git reset实现回退操作。首先需要明确三个区域：工作区/ 暂存区/ Repository。
我们对工作区中的代码进行修改，使用git add .将修改的所有代码添加到暂存区，使用git push将暂存区中的代码提交到远程的Repository。
–hard 撤销工作区所有未提交修改，将暂存区和工作区都回到上一个版本，删除之前的提交信息 –soft 保留工作目录中的内容，把新内容放置在暂存区。 –mixed 所有修改放在工作目录 ⚠️：git reset后再次push会引发冲突，需要push -f 1.</description>
    </item>
    
  </channel>
</rss>
